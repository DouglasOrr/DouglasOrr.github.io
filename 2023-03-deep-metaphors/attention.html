<!DOCTYPE html>
<html lang="en" xmlns:og="http://opengraphprotocol.org/schema/">
  <head>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />
    <meta name="author" content="Douglas Orr" />
    <meta name="keywords" content="deep-learning,metaphors" />
    <meta property="og:type" content="article" />
    <meta property="og:site_name" content="Doug's Diversions" />
    <meta property="og:title" content="Attention">
    <link rel="stylesheet" href="/css/lib.css" />
    <link rel="stylesheet" href="/css/custom.css" />
    <script type="text/javascript" src="/js/custom.js" defer></script>
    <script type="text/javascript" src="/js/lib.js" defer></script>
    <title>Attention</title>
  </head>

  <body>
    <nav class="navbar navbar-dark bg-dark">
      <a class="navbar-brand" href="/">Doug's Diversions</a>
    </nav>
    <div class="container dd-root">
      <div class="row">
        <div class="col"><h1 id="attention">Attention</h1>
<p><strong>The metaphor</strong> <em>You're looking for mentions of your new product WhirlyGigs in a review. Your eyes quickly skim the page, resting only when the familiar pattern of letters is found. When you score a match, you quickly process the surrounding context and adjust your judgement of the review.</em></p>
<p><em>Transformer multi head self attention defines multiple heads. Each head constructs query vectors. It scores each query against a key vectors for each position in a sequence. These are compared and normalised to give importance scores for each position. The importance scores are used in a weighted sum of values, also generated for each position.</em></p>
<p><img alt="A large flock of colorful sheep. Photo. HDR." class="img-fluid" src="img/attention.png" />
<em>Image generated by Stable Diffusion 2.1, prompt: "A large flock of colorful sheep. Photo. HDR."</em></p>
<p><strong>The good</strong> The heart of the connection to the metaphor is the <em>attention map</em>, a normalised weighting function over tokens in a sequence or blocks in an image. It is easy to visualise and amenable to tweaking, and generally behaves as we'd expect. Helpfully, both axes on an attention map are grounded in the input/output of the model. Most importantly, it is consistent with the metaphor—high-weight regions are typically the relevant bits &amp; they do tell us what the model is looking at.</p>
<p><strong>The bad</strong> First, there are a lot of attention maps to choose from. GPT-3 has 96 layers $\times$ 96 heads per layer, a whopping 9216 attention heads in total, each defining their own attention map for any given input. This puts attention-based interpretation in a risky place—if we look hard enough there's probably an attention map to support a new theory about what is happening for a given input.</p>
<p>Another limitation of attention maps is that they tell us nothing about the nature of the <em>interaction</em> between multiple heads, or the values being mixed by the weighted map. Transformer models often look at the same position multiple times, either in parallel heads in the same layer or across layers. In metaphor-land, it's not clear why you'd need to look multiple times.</p>
<p><strong>Conclusion</strong> Attention is a good name. Attention maps are useful tools. They are certainly at risk of over-interpretation, and put you at risk of confirmation bias. As ever—enjoy the metaphor, use it with care.</p>
<hr />
<ul>
<li><a href="article.html">Introduction</a></li>
<li><a href="neural_network.html">A Neural Network</a></li>
<li><a href="loss_landscape.html">The Loss Landscape</a></li>
<li><strong>Attention</strong></li>
<li><a href="mixture_of_experts.html">Mixture of Experts</a></li>
<li>Finally, a <a href="conclusion.html">conclusion</a>, thanks for reading!</li>
</ul></div>
      </div>
      <div class="row dd-footer">
        <div class="col">
          <p>
            Note: All views or opinions expressed here are those of the author
            at time of writing and do not represent those of any employer or
            other organisation, past or present.
          </p>
          <p>
            Please let me know of errors or missing references by
            <a href="https://github.com/DouglasOrr/DouglasOrr.github.io/issues"
              >raising an issue on GitHub</a
            >.
          </p>
        </div>
      </div>
    </div>
  </body>
</html>
